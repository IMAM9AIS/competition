{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imamk\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\imamk\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "#numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#machine learning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve,accuracy_score,mean_squared_log_error\n",
    "from sklearn.metrics import roc_curve, auc,roc_auc_score,mean_squared_error\n",
    "import xgboost as xgb\n",
    "from numpy.linalg import inv\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import matplotlib.dates\n",
    "import pylab as p\n",
    "#import matplotlib.axes3d as p3\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D,LSTM,GlobalMaxPool1D,Dropout,Dense,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new approach - using lemmetization first - kaggle kernel\n",
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word,tag in pos_tag(word_tokenize(sentence)):\n",
    "        if(tag.startswith(\"NN\")):\n",
    "            yield wnl.lemmatize(word,pos= 'n')\n",
    "        elif(tag.startswith(\"VB\")):\n",
    "            yield wnl.lemmatize(word,pos = 'v')\n",
    "        elif(tag.startswith(\"JJ\")):\n",
    "            yield wnl.lemmatize(word,pos = 'a')\n",
    "        elif(tag.startswith(\"R\")):\n",
    "            yield wnl.lemmatize(word,pos = 'r')\n",
    "        else:\n",
    "            yield word\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "Y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test  = test[\"comment_text\"].fillna(\"fillna\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1 =[]\n",
    "X_test1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data lemmatization begins\n",
      "Train data lemmatization ends\n",
      "Test data lemmatization begins\n",
      "Test data lemmatization ends\n"
     ]
    }
   ],
   "source": [
    "print (\"Train data lemmatization begins\")\n",
    "for i in range(0,len(train)):\n",
    "    X_train1.append(\" \".join(lemmatize_all(str(train['comment_text'][i]))))\n",
    "print (\"Train data lemmatization ends\")\n",
    "print (\"Test data lemmatization begins\")\n",
    "for i in range (0, len(test)):\n",
    "    X_test1.append(\" \".join(lemmatize_all(str(test['comment_text'][i]))))\n",
    "print (\"Test data lemmatization ends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "maxlen = 100\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(list(X_train1) + list(X_test1))\n",
    "X_train = tokenizer.texts_to_sequences(X_train1)\n",
    "X_test = tokenizer.texts_to_sequences(X_test1)\n",
    "X_train = sequence.pad_sequences(X_train,maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../crawl-300d-2M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE,encoding=\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features,len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words,embed_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word,i in word_index.items():\n",
    "    if(i>=max_features):continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if(embedding_vector is not None):\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#important stuff to have an evaluation metric\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self,validation_data = (),interval = 1):\n",
    "        super(Callback,self).__init__()\n",
    "        self.interval = interval\n",
    "        self.X_val,self.y_val = validation_data\n",
    "    def on_epoch_end(self,epoch,logs = {}):\n",
    "        if(epoch%self.interval == 0):\n",
    "            y_pred = self.model.predict(self.X_val,verbose = 0)\n",
    "            score = roc_auc_score(self.y_val,y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GlobalAveragePooling1D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(128,dropout=0.1,recurrent_dropout=0.1, return_sequences=True))(x)\n",
    "    x = Conv1D(64,kernel_size=3,padding=\"valid\",kernel_initializer=\"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=\"Adam\",\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imamk\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py:1589: UserWarning: RNN dropout is no longer supported with the Theano backend due to technical limitations. You can either set `dropout` and `recurrent_dropout` to 0, or use the TensorFlow backend.\n",
      "  'RNN dropout is no longer supported with the Theano backend '\n"
     ]
    }
   ],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imamk\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 127\n",
    "epochs = 4\n",
    "X_tra,X_val,Y_tra,y_val = train_test_split(X_train,Y_train,train_size = 0.9,random_state = 233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/4\n",
      "143613/143613 [==============================] - 1022s 7ms/step - loss: 0.0553 - acc: 0.9803 - val_loss: 0.0444 - val_acc: 0.9831\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.986699 \n",
      "\n",
      "Epoch 2/4\n",
      "143613/143613 [==============================] - 1166s 8ms/step - loss: 0.0405 - acc: 0.9844 - val_loss: 0.0441 - val_acc: 0.9833\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986448 \n",
      "\n",
      "Epoch 3/4\n",
      "143613/143613 [==============================] - 1208s 8ms/step - loss: 0.0345 - acc: 0.9864 - val_loss: 0.0461 - val_acc: 0.9828\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.986118 \n",
      "\n",
      "Epoch 4/4\n",
      "143613/143613 [==============================] - 1249s 9ms/step - loss: 0.0287 - acc: 0.9886 - val_loss: 0.0512 - val_acc: 0.9817\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.985080 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_tra, Y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 321s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict(X_test,batch_size=1024,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hamse_na_ho_payega = test[['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for topic in list_classes:\n",
    "    hamse_na_ho_payega[topic] = y_test[:,i]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hamse_na_ho_payega.to_csv('hamse_na_ho_payega.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
